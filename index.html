<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <link rel="icon" type="image/x-icon" href="static/images/mico-logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/mico-logo.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<!-- Paper Title and Author -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ğŸ¨ MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition</h1>
          <!-- <h1 class="title is-1 publication-title" style="font-size: 32pt;"><span style="font-weight: bolder;">ğŸ¨ MICo-150K</span>
            <br><span style="font-weight:normal">A Comprehensive Dataset Advancing Multi-Image Composition</span></h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block corresponding-author">
              <a href="">Xinyu Wei</a><sup>1,4</sup>,&nbsp&nbsp
            </span>
            <span class="author-block corresponding-author">
              <a href="">Kangrui Cen</a><sup>4</sup>,&nbsp&nbsp
            </span>
            <span class="author-block corresponding-author">
              <a href="">Hongyang Wei</a><sup>2,4</sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="">Zhen Guo</a><sup>1,4</sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="">Bairui Li</a><sup>1,4</sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="">Zeqing Wang</a><sup>3,4</sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="">Jinrui Zhang</a><sup>1,4</sup>&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="">Lei Zhang</a><sup>1,4</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 15px;">
            <div style="margin-bottom: 8px;">
              <span class="author-block" style="margin-right: 25px;"><sup>1</sup>Hong Kong Polytechnic Uniersity</span>
              <span class="author-block" style="margin-right: 25px;"><sup>2</sup>Tsinghua University</span>
            </div>
            <div>
              <span class="author-block" style="margin-right: 25px;"><sup>3</sup>Sun Yat-Sen University</span>
              <span class="author-block" style="margin-right: 25px;"><sup>4</sup>OPPO Research Institute</span>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.07348"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Gradio Link. -->
              <!-- <span class="link-block">
                  <a href="http://106.14.2.150:10020/"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="static/images/demo.svg" alt="Model" style="width: 20px; height: 20px;">
                      </span>
                      <span>Demo</span>
                  </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/A113N-W3I/MICo-150K"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- MICo-Data Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
              <!-- MICo-Bench Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                    <img src="static/images/wrench.svg" alt="Gradio Logo" style="width: 18px; height: 18px;">
                    </span>
                  <span>MICo-Bench</span>
                  </a>
              </span>
              <!-- Model Link -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="static/images/model.svg" alt="Model" style="width: 22px; height: 22px;">
                  </span>
                  <span>Model</span>
                  </a>
              </span>
              <!-- Bibtex -->
              <span class="link-block">
                <a href="#BibTeX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-book"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
            </div>

          </div>
            
            
            

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Rolling Banner for examples -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel" style="flex-wrap: nowrap;">
        <div class="item example-2">
          <img src="static/figs/1-1.png" alt="arch" style="max-width: 70%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
        <div class="item example-1">
          <img src="static/figs/1-2.png" alt="arch" style="max-width: 66%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
        <div class="item example-3">
          <img src="static/figs/1-3.png" alt="arch" style="max-width: 72%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
        <div class="item example-4">
          <img src="static/figs/1-4.png" alt="arch" style="max-width: 80%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
        <div class="item example-5">
          <img src="static/figs/1-5.png" alt="arch" style="max-width: 80%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
        <div class="item example-6">
          <img src="static/figs/1-6.png" alt="arch" style="max-width: 80%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
        <div class="item example-7">
          <img src="static/figs/1-7.png" alt="arch" style="max-width: 58%; height: auto; display: block; margin: 0 auto 30px;">
        </div>
      </div>
      <!-- <p class="hero-centered-text"> ... scroll to view more examples ...</p> -->
      <h2 class="title has-text-centered is-size-6" style="font-weight: 400; line-height: 32px; font-style: italic; padding-top: 20px;">
        Qwen-MICo outperforms state-of-the-art Qwen-Image-Edit-2509.
        (Scroll to view more samples)
      </h2>
    </div>
  </div>
</section>
<script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
		<script>
			bulmaCarousel.attach('#results-carousel', {
				slidesToScroll: 1,
				slidesToShow: 1,
        autoplay: true, 
        autoplaySpeed: 3000,
        loop: true
			});
</script>


<!-- Paper Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, 
            i.e., <b>Multi-Image Composition</b> (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data.
            To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts.
            Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in <span class='sphinx-v'>MICo-150K</span>, a comprehensive dataset for MICo with identity consistency.
          </p>
          <p>
            We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions.
            To enable comprehensive evaluation, we construct <span class='sphinx-v'>MICo-Bench</span> with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, <b>Weighted-Ref-VIEScore</b>, specifically tailored for MICo evaluation.
          </p>
          <p>
            Finally, we fine-tune multiple models on <b>MICo-150K</b> and evaluate them on <b>MICo-Bench</b>.
            The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills.
            Notably, our baseline model, <span class="sphinx-v">Qwen-MICo</span>, fine-tuned from Qwen-Image-Edit, matches <b>Qwen-Image-2509</b> in 3-image composition while supporting arbitrary multi-image inputs beyond the latterâ€™s limitation.
            Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths"> 
        
        <h2 class="title is-3">
          <img src="static/images/mico-logo.png" alt="Icon" style="height: 1.1em; vertical-align: middle; margin-right: 0.1em;">
          <span class="sphinx-v">MICo-150K</span>: Advanced Data Construction Pipeline Compared to Previous Methods
        </h2>
        
        <div class="content has-text-justified">
          
          <div class="columns is-vcentered is-variable is-8">
            
            <div class="column is-5 has-text-centered">
              <img src="static/pipeline/other-data-wrong.png" alt="Previous Methods Arch" style="width: 100%; height: auto;">
            </div>

            <div class="column is-7">
              <div class="box" style="background-color: #f5f5f5; border: none; padding: 1.5rem; line-height: 1.6;">
                <p style="font-size: 1.05rem; margin-bottom: 1rem;">
                  Previous MICo methods typically collect high-quality images or video frames as <strong>target images (1)</strong>. 
                  Using <a href="https://github.com/IDEA-Research/GroundingDINO" target="_blank">Open-Vocabulary Detectors</a> (OVD) and <a href="https://github.com/facebookresearch/segment-anything" target="_blank">SAM</a>, objects within targets are segmented to obtain <strong>source images (2)</strong>.
                  Some methods enhance the targets by retrieving additional frames of the same subject from videos <strong>(3)</strong>, 
                  or enhance the sources using S2I (Subject-to-Image) or inpainting models <strong>(4)</strong>.
                </p>
                <p style="font-size: 1.05rem;">
                  Training pairs are then constructed along multiple paths: 
                  <strong style="color: #2980b9;">(2â†’1)</strong>, 
                  <strong style="color: #c0392b;">(2â†’3)</strong>, 
                  <strong style="color: #f39c12;">(4â†’1)</strong>, and 
                  <strong style="color: #27ae60;">(4â†’3)</strong>.
                  However, the masks in <strong>(2)</strong> are often incomplete and semantically ambiguous; the generated images in <strong>(4)</strong> tend to share similar styles, 
                  content, and limited diversity due to reliance on a few fixed generative models; the frames in <strong>(3)</strong> originate from a small number of high-quality videos, 
                  leading to limited scene variety and a lack of imaginative or complex multi-subject scenarios.
                </p>
              </div>
            </div>
          </div>

          <h3 class="title is-4 has-text-centered">Our Data Construction Pipeline</h3>
          
          <div class="has-text-centered" style="margin-bottom: 2rem; margin-top: 2rem;">
            <img src="static/pipeline/our-pipe-1.png" alt="Pipeline Part 1" style="width: 100%; border-radius: 5px;">
          </div>

          <div style="font-size: 1.05rem; line-height: 1.6; margin-bottom: 3rem;">
            <p>
              (a): High-quality open-source data are collected and cleaned through a dedicated pipeline, categorized into four groups: 
              <strong>human</strong>, <strong>object</strong>, <strong>clothes</strong>, and <strong>scene</strong>, each with detailed captions.
              For each task, a diverse set of source images is randomly sampled from these categories, and a multi-image composition prompt is generated by GPT-4o using image captions under a â€œ<strong>Composed-by-Retrieval</strong>â€ strategy.
              The generated prompt is then fed into <a href="https://developers.googleblog.com/en/introducing-gemini-2-5-flash-image/" target="_blank">Nano-Banana</a> to synthesize composite images.
              Each output image undergoes an automated verification process including <a href="https://arxiv.org/abs/2502.13923" target="_blank">QwenVL2.5-72B</a> and <a href="https://github.com/deepinsight/insightface" target="_blank">ArcFace</a> to ensure that all source images are correctly represented in the final composition before being included in the dataset.
            </p>
          </div>

          <div class="has-text-centered" style="margin-bottom: 2rem;">
             <img src="static/pipeline/our-pipe-2.png" alt="Pipeline Part 2" style="width: 100%; border-radius: 5px;">
          </div>

          <div style="font-size: 1.05rem; line-height: 1.6;">
            <p>
              (b): We collect a large number of high-quality single-person portraits through our data-cleaning pipeline and use <a href="https://developers.googleblog.com/en/introducing-gemini-2-5-flash-image/" target="_blank">Nano-Banana</a> to <strong>decompose</strong> each image into its constituent componentsâ€”<strong>scene</strong>, <strong>human</strong>, <strong>objects</strong>, and <strong>clothes</strong>.
              Human annotators then carefully inspect all decomposed components to ensure their quality.
              Once all parts meet the required standards, <strong>Nano-Banana</strong> is used again to <strong>recompose</strong> them into a complete image.
            </p>
          </div>

          <h3 class="title is-4 has-text-centered">Visualization Examples of MICo-150K Dataset</h3>
          
          <div class="has-text-centered" style="margin-bottom: 2rem; margin-top: 2rem;">
            <img src="static/figs/dataset-case-compressed.jpg" alt="dataset-case" style="width: 100%; border-radius: 5px;">
          </div>

          <div style="font-size: 1.05rem; line-height: 1.6; margin-bottom: 3rem;">
            <p>
              <strong>[Row 1] <span style="color: #2980b9;">(Object-Centric)</span>:</strong> â€œ2 objects + sceneâ€ and â€œ4 objectsâ€ compositions.<br>
              <strong>[Row 2] <span style="color: #e67e22;">(Person-Centric)</span>:</strong> â€œ3 womenâ€ and â€œ2 persons + sceneâ€.<br>
              <strong>[Row 3] <span style="color: #c0392b;">(Human-Object Interaction)</span>:</strong> â€œ1 person + 4 objectsâ€ and â€œ2 persons + 2 objectsâ€.<br>
              <strong>[Row 4] <span style="color: #8e44ad;">(De&Re)</span>:</strong> The first image is a <b>real-world</b> photo, the last is the <b>recomposed</b> result, with its <b>intermediate visual elements</b> including decomposed persons, objects, clothes, and scene components.
            </p>
          </div>

        </div> </div> </div> </div> </section>







<!-- MICo Bench -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">

        <h2 class="title is-3">
          <img src="static/images/database.png" alt="Icon" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
          <span class="mdvp-data">MICo-Bench</span>: Comprehensive Evaluation for MICo Tasks
        </h2>

        <div class="content has-text-justified">
          
          <!-- <h3 class="title is-4 has-text-centered" style="margin-top: 2rem; margin-bottom: 1.5rem;">
            Why Existing I2I Metrics Fail for MICo?
          </h3> -->

          <p>
            With the rapid rise of general-purpose Vision-Language Models (VLMs), evaluating generative tasks such as I2I and T2I through VLMs has become increasingly popular.
            <a href="https://tiger-ai-lab.github.io/VIEScore/" target="_blank">VIEScore</a> VIEScore offers a representative framework for VLM-based image evaluation by decomposing overall quality into two dimensions: Semantic Consistency (<strong>SC</strong>) and Perceptual Quality (<strong>PQ</strong>). The final score is computed as <strong>SC Ã— PQ</strong>.
            <a href="https://github.com/VectorSpaceLab/OmniGen2" target="_blank">OmniContext Bench</a> follows this framework and employs <a href="https://arxiv.org/abs/2410.21276" target="_blank">GPT-4o</a> to evaluate the <strong>SC</strong> score of composition results in terms of Prompt Following (<strong>PF</strong>) and Subject Resemblance (<strong>SR</strong>).
          </p>

          <div class="notification is-warning is-light" style="margin-top: 1.5rem; margin-bottom: 1.5rem; text-align: justify;">
            <span class="icon" style="margin-right: 0.5rem;">âš ï¸</span>
            <strong>Why Existing I2I Metrics Fail for MICo?</strong> 
            This evaluation paradigm requires all source images to be provided simultaneously during evaluation. 
            Although modern VLMs such as <b>GPT-4o</b> are highly capable, their <strong>cross-image attention remains limited</strong>.
            When too many images are supplied, the model struggles to accurately perceive each imageâ€™s content, leading to unreliable assessments of composition quality and consequently incorrect scores.
          </div>

          <div class="has-text-centered" style="margin-bottom: 0.5rem;">
            <img src="static/figs/bench.png" 
                 alt="Comparison between traditional VIEScore and MICo-Bench evaluation" 
                 style="width: 100%; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          
          <!-- <p class="is-size-7 has-text-centered" style="color: gray; margin-bottom: 2rem;">
            <strong>Figure 2.</strong> Comparison between the traditional VIEScore evaluation paradigm (left) and our proposed MICo-Bench pipeline (right).
          </p> -->

          <div class="box" style="border-left: 5px solid #ff3860; background-color: #fff5f7;"> <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #ff3860;">
                <span class="icon"><i class="fas fa-times-circle"></i></span> The Issue with Traditional VIEScore
             </h5>
             <p style="font-size: 0.95rem;">
                Traditional VIEScore requires inputting all source images together with the generated image into the evaluator, which often leads to degraded performance as <strong>GPT-4oâ€™s cross-image attention becomes overloaded</strong>. 
                This prevents the model from fully understanding each image and accurately determining whether every source appears in the composed result, resulting in substantial scoring errors.
             </p>
          </div>

          <div class="box" style="border-left: 5px solid #23d160; background-color: #f1fdf5;"> <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #23d160;">
                <span class="icon"><i class="fas fa-check-circle"></i></span> The MICo-Bench Solution
             </h5>
             <p style="font-size: 0.95rem;">
                In the example shown above, all three human evaluators unanimously agreed that Image&nbsp;B was clearly superior.
                In contrast, <strong>MICo-Bench</strong> first assesses whether each individual source image appears in the generated result to produce structured weights. 
                Each test case additionally includes a verified reference image that contains all sources. 
                During evaluation, GPT-4o compares only the generated image with the reference image, enabling reliable, human-level judgment accuracy.
             </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>





<!-- Experiments -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img src="static/images/performance.png" alt="Icon" style="height: 1.4em; vertical-align: middle; margin-right: 0.1em;">
          Enabling Community Models with MICo Capability
        </h2>

        <p style="text-align: left; margin-bottom: 20px;">
          To validate the effectiveness of our dataset, we train five open-source models: <a href="https://bagel-ai.org/" target="_blank">BAGEL</a>, 
          <a href="https://github.com/VectorSpaceLab/OmniGen2" target="_blank">OmniGen2</a>, <a href="https://github.com/Alpha-VLLM/Lumina-DiMOO" target="_blank">Luminia-DiMOO</a>, 
          <a href="https://github.com/JiuhaiChen/BLIP3o" target="_blank">BLIP3-o</a>, <a href="https://github.com/QwenLM/Qwen-Image" target="_blank">Qwen-Image-Edit</a> on MICo-150K, 
          and conduct a comprehensive evaluation of their performance.
        </p>

        <h3 class="title is-4 has-text-centered">MICo-Bench Qualitative Results</h3>
        <div class="content has-text-justified">
          <div style="text-align: center; margin-bottom: 20px;">
            <img src="static/figs/performance.png" alt="arch" style="height: auto; width: 90%; display: inline-block;">
            <p>
              Performance comparison on <b>MICo-Bench</b> across different open-source and closed-source model:
              â€œbaseâ€ denotes the original model; â€œw/oâ€ indicates fine-tuning without the De&Re task;
              â€œrealâ€ and â€œsynthâ€ correspond to fine-tuning with real and synthetic compositions from the De&Re task, respectively.
              The best performance of each model under each task is highlighted in <b>bold</b>.
            </p>
          </div>

          <div style="text-align: center; margin-bottom: 20px;">
            <img src="static/figs/train-case.png" alt="arch" style="height: auto; width: 90%; display: inline-block;">
            <p>
              The leftmost displays the source and reference images. The first row shows model outputs before fine-tuning, the second row presents outputs after fine-tuning. 
              The Weighted-Ref-VIEScore for each generated result is annotated in the corner. MICo-150K demonstrates strong robustness: 
              <b>BLIP-3o</b> and <b>Lumina-DiMOO</b> acquire MICo capability from scratch; the emergent MICo abilities of <b>BAGEL</b> and <b>Qwen-Image</b>
              are significantly strengthened; <b>OmniGen2</b> achieves further improvement on top of its already strong performance.
            </p>
          </div>
        </div>


        <h3 class="title is-4 has-text-centered">Qualitative Comparisons Before & After SFT on MICo-150K</h3>
        <div class="content has-text-justified">
          <div style="text-align: center; margin-bottom: 20px;">
            <img src="static/figs/train-cases.png" alt="arch" style="height: auto; width: 90%; display: inline-block;">
            <p>
              Comparison of open-source models before and after MICo-150K training. Some source images were cropped or background-removed for visualization. 
              <b>BLIP3-o</b> and <b>Luminia-DiMOO</b> gain strong multi-image composition abilities after training. 
              <b>Qwen-Image-Edit</b> and <b>BAGEL</b> were not explicitly trained for MICo tasks, 
              but exhibit emergent MICo capabilities that are further enhanced through fine-tuning. 
              <b>OmniGen2</b> preserves identity well and produces more aesthetic, prompt-aligned results after training.
            </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section> -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        
        <h2 class="title is-3">
          <img src="static/images/model.png" alt="Icon" style="height: 1.4em; vertical-align: middle; margin-right: 0.1em;">
          Enabling Community Models with MICo Capability
        </h2>

        <p style="text-align: left; margin-bottom: 30px; font-size: 1.1em;">
          To validate the effectiveness of our dataset, we train five open-source models: 
          <a href="https://bagel-ai.org/" target="_blank">BAGEL</a>, 
          <a href="https://github.com/VectorSpaceLab/OmniGen2" target="_blank">OmniGen2</a>, 
          <a href="https://github.com/Alpha-VLLM/Lumina-DiMOO" target="_blank">Lumina-DiMOO</a>, 
          <a href="https://github.com/JiuhaiChen/BLIP3o" target="_blank">BLIP3-o</a>, 
          <a href="https://github.com/QwenLM/Qwen-Image" target="_blank">Qwen-Image-Edit</a> on MICo-150K, 
          and conduct a comprehensive evaluation of their performance.
        </p>

        <h3 class="title is-4 has-text-centered" style="margin-bottom: 20px;">
          <img src="static/images/tradition-bench.png" alt="Icon" style="height: 1.4em; vertical-align: middle; margin-right: 0.1em;">
          MICo-Bench Qualitative Results
        </h3>
        
        <div class="content">
          
          <div style="text-align: center; margin-bottom: 40px;">
            <img src="static/figs/performance.png" alt="performance comparison" style="height: auto; width: 90%; display: inline-block; border-radius: 5px;">
            <p style="text-align: left; margin-top: 10px;">
              Performance comparison on <b>MICo-Bench</b> across different open-source and closed-source models:
              â€œbaseâ€ denotes the original model; â€œw/oâ€ indicates fine-tuning without the De&Re task;
              â€œrealâ€ and â€œsynthâ€ correspond to fine-tuning with real and synthetic compositions from the De&Re task, respectively.
              The best performance of each model under each task is highlighted in <b>bold</b>.
            </p>
          </div>

          <div style="text-align: center; margin-bottom: 40px;">
            <img src="static/figs/train-case.png" alt="training cases" style="height: auto; width: 90%; display: inline-block; border-radius: 5px;">
            <p style="text-align: left; margin-top: 10px;">
              The leftmost displays the source and reference images. The first row shows model outputs before fine-tuning, the second row presents outputs after fine-tuning. 
              The Weighted-Ref-VIEScore for each generated result is annotated in the corner. MICo-150K demonstrates strong robustness: 
              <b>BLIP-3o</b> and <b>Lumina-DiMOO</b> acquire MICo capability from scratch; the emergent MICo abilities of <b>BAGEL</b> and <b>Qwen-Image</b>
              are significantly strengthened; <b>OmniGen2</b> achieves further improvement on top of its already strong performance.
            </p>
          </div>
        </div>

        <h3 class="title is-4 has-text-centered" style="margin-bottom: 20px;">Qualitative Comparisons Before & After SFT on MICo-150K</h3>
        
        <div class="content">
          <div style="text-align: center; margin-bottom: 20px;">
            <img src="static/figs/train-cases-compressed.jpg" alt="qualitative comparisons" style="height: auto; width: 90%; display: inline-block; border-radius: 5px;">
            <p style="text-align: left; margin-top: 10px;">
              Comparison of open-source models before and after MICo-150K training. Some source images were cropped or background-removed for visualization. 
              <b>BLIP3-o</b> and <b>Lumina-DiMOO</b> gain strong multi-image composition abilities after training. 
              <b>Qwen-Image-Edit</b> and <b>BAGEL</b> were not explicitly trained for MICo tasks, 
              but exhibit emergent MICo capabilities that are further enhanced through fine-tuning. 
              <b>OmniGen2</b> preserves identity well and produces more aesthetic, prompt-aligned results after training.
            </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>






<section class="section" id="More-Results">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          More Qualitative Results on MICo-Bench
        </h2>
        <p class="subtitle is-5 has-text-grey">
          Click the button below to view the corresponding generation results before and after SFT (or compared with Qwen-2509).
        </p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full">
        
        <div class="box" style="background-color: #fcfcfc; border: 1px solid #eee; padding: 10px; min-height: 400px; display: flex; align-items: center; justify-content: center;">
          <img id="main-gallery-image" 
               src="static/more-results/1.png" 
               alt="Result Display" 
               style="width: 100%; height: auto; border-radius: 5px; transition: opacity 0.3s ease;">
        </div>

        <div class="fingerprint-container has-text-centered" style="margin-top: 20px;">
          <div style="display: flex; justify-content: center; gap: 15px; flex-wrap: wrap;">
            
            <div class="fingerprint-item is-active" onclick="switchGallery(1, this)">
              <img src="static/GT/1.png" alt="ID 1">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(2, this)">
              <img src="static/GT/2.png" alt="ID 2">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(3, this)">
              <img src="static/GT/3.png" alt="ID 3">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(4, this)">
              <img src="static/GT/4.png" alt="ID 4">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(5, this)">
              <img src="static/GT/5.png" alt="ID 5">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(6, this)">
              <img src="static/GT/6.png" alt="ID 6">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(7, this)">
              <img src="static/GT/7.png" alt="ID 7">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(8, this)">
              <img src="static/GT/8.png" alt="ID 8">
            </div>

            <div class="fingerprint-item" onclick="switchGallery(9, this)">
              <img src="static/GT/9.png" alt="ID 9">
            </div>

          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<style>
  /* æŒ‡çº¹æŒ‰é’®çš„åŸºç¡€æ ·å¼ */
  .fingerprint-item {
    width: 80px;  /* è®¾ç½®æŒ‰é’®å®½åº¦ */
    height: 80px; /* è®¾ç½®æŒ‰é’®é«˜åº¦ */
    border-radius: 12px; /* åœ†è§’çŸ©å½¢ */
    overflow: hidden;
    cursor: pointer;
    border: 3px solid transparent; /* é»˜è®¤é€æ˜è¾¹æ¡†ï¼Œé˜²æ­¢hoveræ—¶æŠ–åŠ¨ */
    transition: all 0.2s ease;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    opacity: 0.7; /* æœªé€‰ä¸­æ—¶ç¨å¾®é€æ˜ä¸€ç‚¹ */
  }

  .fingerprint-item img {
    width: 100%;
    height: 100%;
    object-fit: cover; /* ä¿è¯å›¾ç‰‡å¡«æ»¡æ–¹æ¡†ä¸”ä¸å˜å½¢ */
  }

  /* é¼ æ ‡æ‚¬åœæ ·å¼ */
  .fingerprint-item:hover {
    transform: translateY(-3px); /* å¾®å¾®ä¸Šæµ® */
    opacity: 1.0;
    box-shadow: 0 5px 15px rgba(0,0,0,0.2);
  }

  /* é€‰ä¸­çŠ¶æ€ (Active) */
  .fingerprint-item.is-active {
    border-color: #2980b9; /* é€‰ä¸­æ—¶çš„è¾¹æ¡†é¢œè‰²ï¼ˆè“è‰²ï¼‰ */
    opacity: 1.0;
    box-shadow: 0 0 0 3px rgba(41, 128, 185, 0.3); /* å¤–å‘å…‰æ•ˆæœ */
    transform: scale(1.05);
  }
</style>

<script>
  function switchGallery(index, element) {
    // 1. è·å–å¤§å›¾å…ƒç´ 
    var mainImage = document.getElementById('main-gallery-image');
    
    // 2. ç®€å•çš„æ·¡å‡ºæ·¡å…¥æ•ˆæœ
    mainImage.style.opacity = 0;

    setTimeout(function() {
      // åˆ‡æ¢å›¾ç‰‡è·¯å¾„
      mainImage.src = 'static/more-results/' + index + '.png';
      // æ¢å¤ä¸é€æ˜åº¦
      mainImage.style.opacity = 1;
    }, 200); // 200ms ååˆ‡æ¢ï¼Œé…åˆCSS transition

    // 3. å¤„ç†é«˜äº®çŠ¶æ€
    // ç§»é™¤æ‰€æœ‰æŒ‰é’®çš„ .is-active ç±»
    var items = document.querySelectorAll('.fingerprint-item');
    items.forEach(function(item) {
      item.classList.remove('is-active');
    });

    // ç»™å½“å‰ç‚¹å‡»çš„æŒ‰é’®æ·»åŠ  .is-active ç±»
    element.classList.add('is-active');
  }

  // å¯é€‰ï¼šé¢„åŠ è½½æ‰€æœ‰å¤§å›¾ï¼Œé˜²æ­¢ç¬¬ä¸€æ¬¡ç‚¹å‡»æ—¶é—ªçƒ
  window.onload = function() {
    for (var i = 1; i <= 9; i++) {
      var img = new Image();
      img.src = 'static/more-results/' + i + '.png';
    }
  };
</script>





<section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">
          <img src="static/images/performance.png" alt="Icon" style="height: 1.4em; vertical-align: middle; margin-right: 0.1em;">
          Data Analysis
        </h2>
      </div>
    </div>

    <div class="columns is-centered is-variable is-8">
      
      <div class="column is-6">
        <div class="content">
          <div class="has-text-centered" style="margin-bottom: 1rem;">
            <img src="static/analysis/bar.png" 
                 alt="Dataset Statistics" 
                 style="width: 100%; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <p class="has-text-justified" style="font-size: 0.95rem;">
            <strong>Dataset Statistics:</strong> 
            The statistics of the datasetâ€™s source images and text prompts, demonstrating significantly 
            greater diversity compared to other datasets with similar functionality.
          </p>
        </div>
      </div>

      <div class="column is-6">
        <div class="content">
          <div class="has-text-centered" style="margin-bottom: 1rem;">
            <img src="static/analysis/pie.png" 
                 alt="Semantic Redundancy" 
                 style="width: 100%; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <p class="has-text-justified" style="font-size: 0.95rem;">
            <strong>Semantic Redundancy:</strong> 
            Semantic redundancy in text prompts. We extract text embeddings using CLIP and set a 
            cosine similarity threshold of 0.85 to identify duplicates.
          </p>
        </div>
      </div>

    </div>
    </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@misc{wei2025mico150kcomprehensivedatasetadvancing,
      title={MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition}, 
      author={Xinyu Wei and Kangrui Cen and Hongyang Wei and Zhen Guo and Bairui Li and Zeqing Wang and Jinrui Zhang and Lei Zhang},
      year={2025},
      eprint={2512.07348},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2512.07348}, 
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website design was inspired by <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> , deeply grateful for their great work! 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
